"""Standalone Snake game with optional Double DQN training for Python's IDLE."""

from __future__ import annotations

import argparse
import math
import random
import sys
import time
from collections import deque
from dataclasses import dataclass, field
from functools import partial
from pathlib import Path
from typing import Deque, Dict, Iterable, List, Optional, Set, Tuple

try:  # NumPy krÃ¤vs fÃ¶r Double DQN-implementationen
    import numpy as np
except ModuleNotFoundError as exc:  # pragma: no cover - tydligare fel i IDLE
    raise SystemExit(
        "Det hÃ¤r skriptet krÃ¤ver NumPy. Installera det via 'pip install numpy' "
        "innan du trÃ¤nar eller anvÃ¤nder autopiloten."
    ) from exc

import tkinter as tk
from tkinter import filedialog, messagebox

# ---------------------------------------------------------------------------
# Game configuration
# ---------------------------------------------------------------------------
GRID_SIZE = 15  # Width and height in tiles
CELL_SIZE = 32  # Pixels per tile
STEP_DELAY = 120  # Milliseconds between snake moves
START_LENGTH = 3

# Reward configuration mirroring the web project defaults


@dataclass
class RewardConfig:
    step_penalty: float = 0.01
    turn_penalty: float = 0.001
    approach_bonus: float = 0.03
    retreat_penalty: float = 0.03
    loop_penalty: float = 0.5
    tight_loop_penalty: float = 1.2
    revisit_penalty: float = 0.05
    dead_end_penalty: float = 0.5
    wall_penalty: float = 10.0
    self_penalty: float = 25.5
    timeout_penalty: float = 5.0
    fruit_reward: float = 10.0
    growth_bonus: float = 1.0
    compact_weight: float = 0.0
    compact_bonus: float = 0.25
    trap_penalty: float = 1.2
    space_gain_bonus: float = 0.05


REWARD_BREAKDOWN_KEYS: Tuple[str, ...] = (
    "stepPenalty",
    "turnPenalty",
    "approachBonus",
    "retreatPenalty",
    "loopPenalty",
    "tightLoopPenalty",
    "revisitPenalty",
    "deadEndPenalty",
    "trapPenalty",
    "spaceGainBonus",
    "fruitReward",
    "growthBonus",
    "compactness",
    "wallPenalty",
    "selfPenalty",
    "timeoutPenalty",
)

VISIT_DECAY = 0.995
LOOP_PATTERNS = {(1, 2, 1, 2), (2, 1, 2, 1)}

Direction = Tuple[int, int]
Point = Tuple[int, int]

DIRECTIONS: Dict[str, Direction] = {
    "Up": (0, -1),
    "Down": (0, 1),
    "Left": (-1, 0),
    "Right": (1, 0),
}
ACTION_VECTORS: Tuple[Direction, ...] = (
    (0, -1),
    (1, 0),
    (0, 1),
    (-1, 0),
)
OPPOSITE: Dict[Direction, Direction] = {
    (0, -1): (0, 1),
    (0, 1): (0, -1),
    (-1, 0): (1, 0),
    (1, 0): (-1, 0),
}
DIRECTION_TO_INDEX: Dict[Direction, int] = {vec: idx for idx, vec in enumerate(ACTION_VECTORS)}


# ---------------------------------------------------------------------------
# Shared helpers
# ---------------------------------------------------------------------------

def grid_size_state_size(grid_size: int) -> int:
    return grid_size * grid_size * 2 + len(ACTION_VECTORS)


def build_state_vector(
    snake: Iterable[Point],
    fruit: Point,
    direction_index: int,
    grid_size: int,
) -> np.ndarray:
    """Flattened observation used both for training and the Tk autopilot."""

    snake_grid = np.zeros((grid_size, grid_size), dtype=np.float32)
    fruit_grid = np.zeros_like(snake_grid)
    for x, y in snake:
        if 0 <= x < grid_size and 0 <= y < grid_size:
            snake_grid[y, x] = 1.0
    fx, fy = fruit
    if 0 <= fx < grid_size and 0 <= fy < grid_size:
        fruit_grid[fy, fx] = 1.0
    direction_one_hot = np.zeros(len(ACTION_VECTORS), dtype=np.float32)
    direction_one_hot[direction_index] = 1.0
    return np.concatenate((snake_grid.flatten(), fruit_grid.flatten(), direction_one_hot))


# ---------------------------------------------------------------------------
# Double DQN training components
# ---------------------------------------------------------------------------


@dataclass
class Transition:
    state: np.ndarray
    action: int
    reward: float
    next_state: np.ndarray
    done: bool


class ReplayBuffer:
    """Basic replay buffer for off-policy training."""

    def __init__(self, capacity: int, rng: random.Random) -> None:
        self.capacity = capacity
        self.memory: Deque[Transition] = deque(maxlen=capacity)
        self.rng = rng

    def add(self, transition: Transition) -> None:
        self.memory.append(transition)

    def sample(self, batch_size: int) -> Transition:
        batch = self.rng.sample(self.memory, batch_size)
        states = np.stack([t.state for t in batch])
        actions = np.array([t.action for t in batch], dtype=np.int64)
        rewards = np.array([t.reward for t in batch], dtype=np.float32)
        next_states = np.stack([t.next_state for t in batch])
        dones = np.array([t.done for t in batch], dtype=np.float32)
        return Transition(states, actions, rewards, next_states, dones)

    def __len__(self) -> int:
        return len(self.memory)


class MLP:
    """Small fully connected network implemented with NumPy."""

    def __init__(self, layer_sizes: List[int], seed: Optional[int] = None) -> None:
        self.layer_sizes = layer_sizes
        self.rng = np.random.default_rng(seed)
        self.weights: List[np.ndarray] = []
        self.biases: List[np.ndarray] = []
        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):
            limit = math.sqrt(6.0 / (in_size + out_size))
            weight = self.rng.uniform(-limit, limit, size=(in_size, out_size)).astype(np.float32)
            bias = np.zeros(out_size, dtype=np.float32)
            self.weights.append(weight)
            self.biases.append(bias)

    def forward(self, inputs: np.ndarray, return_cache: bool = False):
        x = inputs.astype(np.float32)
        activations = [x]
        pre_activations = []
        for index, (weight, bias) in enumerate(zip(self.weights, self.biases)):
            x = x @ weight + bias
            pre_activations.append(x)
            if index < len(self.weights) - 1:
                x = np.maximum(x, 0.0)
            activations.append(x)
        if return_cache:
            return x, (activations, pre_activations)
        return x

    def predict(self, inputs: np.ndarray) -> np.ndarray:
        return self.forward(inputs, return_cache=False)

    def backward(self, grad_output: np.ndarray, cache, learning_rate: float) -> None:
        activations, pre_activations = cache
        grad = grad_output
        for layer in reversed(range(len(self.weights))):
            a_prev = activations[layer]
            weight = self.weights[layer]
            grad_w = a_prev.T @ grad
            grad_b = grad.sum(axis=0)
            if layer > 0:
                grad = grad @ weight.T
                grad = grad * (pre_activations[layer - 1] > 0)
            self.weights[layer] -= learning_rate * grad_w
            self.biases[layer] -= learning_rate * grad_b

    def copy(self) -> "MLP":
        clone = MLP(self.layer_sizes)
        clone.weights = [weight.copy() for weight in self.weights]
        clone.biases = [bias.copy() for bias in self.biases]
        return clone


class DoubleDQNAgent:
    """Double DQN agent implemented specifically for the IDLE snake grid."""

    def __init__(
        self,
        state_size: int,
        action_size: int,
        *,
        hidden_layers: Optional[List[int]] = None,
        learning_rate: float = 5e-4,
        gamma: float = 0.99,
        epsilon_start: float = 1.0,
        epsilon_end: float = 0.05,
        epsilon_decay: float = 0.995,
        batch_size: int = 64,
        buffer_capacity: int = 50_000,
        target_sync_interval: int = 500,
        seed: Optional[int] = None,
    ) -> None:
        self.state_size = state_size
        self.action_size = action_size
        self.hidden_layers = hidden_layers or [128, 128]
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_min = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_sync_interval = target_sync_interval
        self.train_steps = 0

        rng_seed = seed if seed is not None else random.randrange(1_000_000)
        self.random = random.Random(rng_seed)
        self.layer_sizes = [state_size, *self.hidden_layers, action_size]
        self.online = MLP(self.layer_sizes, seed=rng_seed)
        self.target = self.online.copy()
        self.buffer = ReplayBuffer(buffer_capacity, self.random)

    # ------------------------------------------------------------------
    # Persistence utilities
    # ------------------------------------------------------------------
    def save(self, path: Path | str) -> Path:
        path = Path(path)
        arrays = {
            "layer_sizes": np.array(self.layer_sizes, dtype=np.int64),
            "learning_rate": np.array([self.learning_rate], dtype=np.float32),
            "gamma": np.array([self.gamma], dtype=np.float32),
            "epsilon": np.array([self.epsilon], dtype=np.float32),
            "epsilon_min": np.array([self.epsilon_min], dtype=np.float32),
            "epsilon_decay": np.array([self.epsilon_decay], dtype=np.float32),
            "batch_size": np.array([self.batch_size], dtype=np.int64),
            "target_sync": np.array([self.target_sync_interval], dtype=np.int64),
        }
        for idx, weight in enumerate(self.online.weights):
            arrays[f"W{idx}"] = weight
        for idx, bias in enumerate(self.online.biases):
            arrays[f"b{idx}"] = bias
        np.savez_compressed(path, **arrays)
        return path

    @classmethod
    def load(cls, path: Path | str) -> "DoubleDQNAgent":
        data = np.load(Path(path), allow_pickle=False)
        layer_sizes = data["layer_sizes"].astype(np.int64).tolist()
        state_size = int(layer_sizes[0])
        action_size = int(layer_sizes[-1])
        hidden_layers = [int(x) for x in layer_sizes[1:-1]]
        agent = cls(
            state_size,
            action_size,
            hidden_layers=hidden_layers,
            learning_rate=float(data["learning_rate"][0]),
            gamma=float(data["gamma"][0]),
            epsilon_start=float(data["epsilon"][0]),
            epsilon_end=float(data["epsilon_min"][0]),
            epsilon_decay=float(data["epsilon_decay"][0]),
            batch_size=int(data["batch_size"][0]),
            target_sync_interval=int(data["target_sync"][0]),
        )
        for idx in range(len(agent.online.weights)):
            agent.online.weights[idx] = data[f"W{idx}"]
            agent.online.biases[idx] = data[f"b{idx}"]
        agent.target = agent.online.copy()
        return agent

    # ------------------------------------------------------------------
    # Interaction helpers
    # ------------------------------------------------------------------
    def select_action(self, state: np.ndarray, greedy: bool = False) -> int:
        if state.ndim == 1:
            state = state[None, :]
        if not greedy and self.random.random() < self.epsilon:
            return self.random.randrange(self.action_size)
        q_values = self.online.predict(state)
        if q_values.ndim == 2:
            q_values = q_values[0]
        return int(np.argmax(q_values))

    def push(self, transition: Transition) -> None:
        self.buffer.add(transition)

    def decay_epsilon(self) -> None:
        if self.epsilon > self.epsilon_min:
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def update_target(self) -> None:
        self.target = self.online.copy()

    def learn(self) -> Optional[float]:
        if len(self.buffer) < self.batch_size:
            return None
        batch = self.buffer.sample(self.batch_size)
        states = batch.state
        actions = batch.action
        rewards = batch.reward
        next_states = batch.next_state
        dones = batch.done

        q_values, cache = self.online.forward(states, return_cache=True)
        q_next_online = self.online.predict(next_states)
        q_next_target = self.target.predict(next_states)
        next_actions = np.argmax(q_next_online, axis=1)
        targets = rewards + (1.0 - dones) * self.gamma * q_next_target[np.arange(self.batch_size), next_actions]

        diff = q_values[np.arange(self.batch_size), actions] - targets
        loss = float(np.mean(diff ** 2) * 0.5)
        grad_output = np.zeros_like(q_values)
        grad_output[np.arange(self.batch_size), actions] = diff / self.batch_size
        self.online.backward(grad_output, cache, self.learning_rate)

        self.train_steps += 1
        if self.train_steps % self.target_sync_interval == 0:
            self.update_target()
        return loss


class IdleSnakeEnv:
    """Lightweight snake environment for headless Double DQN training."""

    def __init__(
        self,
        grid_size: int = GRID_SIZE,
        seed: Optional[int] = None,
        reward_config: Optional[RewardConfig] = None,
    ) -> None:
        self.grid_size = grid_size
        self.random = random.Random(seed)
        self.reward_config = reward_config or RewardConfig()
        self.state = np.zeros(grid_size_state_size(grid_size), dtype=np.float32)
        self.total_cells = grid_size * grid_size
        self.snake: List[Point] = []
        self.snake_set: Set[Point] = set()
        self.direction_index: int = 1
        self.fruit: Point = (0, 0)
        self.pending_growth = 0
        self.steps_since_fruit = 0
        self.total_reward = 0.0
        self.max_length = START_LENGTH
        self.prev_slack = 0.0
        self.last_slack_delta = 0.0
        self.last_free_space_ratio = 1.0
        self.relative_history: Deque[int] = deque(maxlen=6)
        self.head_history: Deque[Point] = deque(maxlen=12)
        self.freedom_history: Deque[float] = deque(maxlen=20)
        self.visit_map: List[List[float]] = []
        self.episode_breakdown = self._make_reward_breakdown()
        self.steps_taken = 0
        self.fruits_eaten = 0
        self.reward_breakdown = self._make_reward_breakdown()

    @property
    def state_size(self) -> int:
        return grid_size_state_size(self.grid_size)

    def _make_reward_breakdown(self) -> Dict[str, float]:
        breakdown = {key: 0.0 for key in REWARD_BREAKDOWN_KEYS}
        breakdown["total"] = 0.0
        return breakdown

    def _decay_visits(self) -> None:
        if not self.visit_map:
            return
        for y in range(self.grid_size):
            row = self.visit_map[y]
            for x in range(self.grid_size):
                row[x] *= VISIT_DECAY

    def _relative_action(self, previous: int, current: int) -> int:
        if current == previous:
            return 0
        if current == (previous - 1) % len(ACTION_VECTORS):
            return 1  # left turn
        if current == (previous + 1) % len(ACTION_VECTORS):
            return 2  # right turn
        return 0

    def _free_space_from(self, start: Point, tail_will_move: bool) -> int:
        blocked = set(self.snake_set)
        blocked.discard(start)
        if tail_will_move and self.snake:
            blocked.discard(self.snake[-1])
        seen: Set[Point] = set()
        queue: List[Point] = [start]
        while queue:
            x, y = queue.pop()
            if (x, y) in seen or (x, y) in blocked:
                continue
            seen.add((x, y))
            for nx, ny in ((x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)):
                if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:
                    queue.append((nx, ny))
            if len(seen) > self.total_cells:
                break
        return len(seen)

    def _compute_slack(self) -> float:
        if not self.snake:
            return 0.0
        xs = [segment[0] for segment in self.snake]
        ys = [segment[1] for segment in self.snake]
        width = max(xs) - min(xs) + 1
        height = max(ys) - min(ys) + 1
        area = width * height
        return max(0.0, float(area - len(self.snake)))

    def reset(self) -> np.ndarray:
        start_x = self.grid_size // 2
        start_y = self.grid_size // 2
        self.snake = [(start_x - i, start_y) for i in range(START_LENGTH)]
        self.snake_set = set(self.snake)
        self.direction_index = 1
        self.pending_growth = 0
        self.steps_since_fruit = 0
        self.total_reward = 0.0
        self.steps_taken = 0
        self.fruits_eaten = 0
        self.max_length = len(self.snake)
        self.prev_slack = self._compute_slack()
        self.last_slack_delta = 0.0
        self.last_free_space_ratio = 1.0
        self.relative_history.clear()
        self.head_history.clear()
        self.freedom_history.clear()
        self.visit_map = [[0.0 for _ in range(self.grid_size)] for _ in range(self.grid_size)]
        self.episode_breakdown = self._make_reward_breakdown()
        self.reward_breakdown = self._make_reward_breakdown()
        self._spawn_fruit()
        self.state = build_state_vector(self.snake, self.fruit, self.direction_index, self.grid_size)
        return self.state.copy()

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[str, object]]:
        if action < 0 or action >= len(ACTION_VECTORS):
            raise ValueError("Action out of range")

        current_direction = ACTION_VECTORS[self.direction_index]
        chosen_direction = ACTION_VECTORS[action]
        if chosen_direction == OPPOSITE[current_direction]:
            chosen_direction = current_direction
            action = self.direction_index
        new_direction_index = DIRECTION_TO_INDEX[chosen_direction]
        relative_action = self._relative_action(self.direction_index, new_direction_index)
        self.direction_index = new_direction_index

        head_x, head_y = self.snake[0]
        dx, dy = chosen_direction
        new_head = (head_x + dx, head_y + dy)

        fx, fy = self.fruit
        prev_distance = abs(head_x - fx) + abs(head_y - fy)
        will_grow = new_head == self.fruit or self.pending_growth > 0
        hits_wall = not (0 <= new_head[0] < self.grid_size and 0 <= new_head[1] < self.grid_size)
        body_to_check = self.snake if will_grow else self.snake[:-1]
        hits_self = new_head in body_to_check
        space_before = self._free_space_from(self.snake[0], tail_will_move=self.pending_growth == 0)

        reward = 0.0
        done = False
        cause = "step"
        step_breakdown = self._make_reward_breakdown()

        if hits_wall or hits_self:
            penalty = self.reward_config.wall_penalty if hits_wall else self.reward_config.self_penalty
            reward -= penalty
            key = "wallPenalty" if hits_wall else "selfPenalty"
            step_breakdown[key] -= penalty
            cause = "wall" if hits_wall else "self"
            done = True
        else:
            self._decay_visits()
            self.snake.insert(0, new_head)
            self.snake_set.add(new_head)
            ate_fruit = new_head == self.fruit
            if ate_fruit:
                self.pending_growth += 1
                self.fruits_eaten += 1
                reward += self.reward_config.fruit_reward
                step_breakdown["fruitReward"] += self.reward_config.fruit_reward
                self.steps_since_fruit = 0
                self._spawn_fruit()
                cause = "fruit"
            else:
                self.steps_since_fruit += 1
                if self.pending_growth > 0:
                    self.pending_growth -= 1
                else:
                    tail = self.snake.pop()
                    self.snake_set.discard(tail)

            nx, ny = new_head
            revisit_penalty = self.visit_map[ny][nx] * self.reward_config.revisit_penalty
            if revisit_penalty:
                reward -= revisit_penalty
                step_breakdown["revisitPenalty"] -= revisit_penalty
            self.visit_map[ny][nx] = min(1.0, self.visit_map[ny][nx] + 0.3)

            reward -= self.reward_config.step_penalty
            step_breakdown["stepPenalty"] -= self.reward_config.step_penalty

            if relative_action in (1, 2):
                reward -= self.reward_config.turn_penalty
                step_breakdown["turnPenalty"] -= self.reward_config.turn_penalty

            new_distance = abs(new_head[0] - fx) + abs(new_head[1] - fy)
            if new_distance < prev_distance:
                reward += self.reward_config.approach_bonus
                step_breakdown["approachBonus"] += self.reward_config.approach_bonus
            elif new_distance > prev_distance:
                reward -= self.reward_config.retreat_penalty
                step_breakdown["retreatPenalty"] -= self.reward_config.retreat_penalty

            space_after = self._free_space_from(new_head, tail_will_move=self.pending_growth == 0)
            need = len(self.snake) + 2
            denom = max(1, need)

            if space_after < need:
                penalty = self.reward_config.trap_penalty * (1.0 + (need - space_after) / denom)
                reward -= penalty
                step_breakdown["trapPenalty"] -= penalty
            elif self.reward_config.space_gain_bonus > 0 and space_after > space_before:
                bonus = self.reward_config.space_gain_bonus * min(1.0, (space_after - space_before) / denom)
                reward += bonus
                step_breakdown["spaceGainBonus"] += bonus

            margin = 5
            min_reachable = len(self.snake) + (1 if self.pending_growth > 0 else 0) + margin
            if min_reachable > 0:
                ratio = space_after / max(1, min_reachable)
                base = -0.5 * (1.0 - ratio)
                if base:
                    dead_end_reward = base * self.reward_config.dead_end_penalty
                    reward += dead_end_reward
                    step_breakdown["deadEndPenalty"] += dead_end_reward

            freedom_ratio = max(0.0, min(1.0, space_after / max(1, self.total_cells)))
            self.freedom_history.append(freedom_ratio)
            if len(self.freedom_history) > self.freedom_history.maxlen:
                self.freedom_history.popleft()
            avg_freedom = sum(self.freedom_history) / len(self.freedom_history)
            trend = freedom_ratio - (self.freedom_history[0] if self.freedom_history else freedom_ratio)

            if trend < -0.05 and avg_freedom < 0.15:
                long_term_penalty = -self.reward_config.trap_penalty * abs(trend) * 4.0
                reward += long_term_penalty
                step_breakdown["trapPenalty"] += long_term_penalty
            if trend > 0.03 and avg_freedom > 0.2:
                long_term_bonus = self.reward_config.compact_bonus * trend * 5.0
                reward += long_term_bonus
                step_breakdown["compactness"] += long_term_bonus

            prev_ratio = self.last_free_space_ratio
            drop = max(0.0, prev_ratio - freedom_ratio)
            penalty_factor = 0.0
            if len(self.snake) > 4 and new_head in self.head_history:
                penalty_factor += 1.0
            if drop > 0.0:
                penalty_factor += min(1.5, drop * 12.0)
            if penalty_factor > 0.0 and self.reward_config.tight_loop_penalty != 0.0:
                loop_penalty = self.reward_config.tight_loop_penalty * penalty_factor
                reward -= loop_penalty
                step_breakdown["tightLoopPenalty"] -= loop_penalty
            self.last_free_space_ratio = freedom_ratio

            self.head_history.append(new_head)
            if len(self.head_history) > self.head_history.maxlen:
                self.head_history.popleft()

            self.relative_history.append(relative_action)
            if len(self.relative_history) > self.relative_history.maxlen:
                self.relative_history.popleft()
            if len(self.relative_history) >= 4:
                last_four = tuple(list(self.relative_history)[-4:])
                if last_four in LOOP_PATTERNS:
                    reward -= self.reward_config.loop_penalty
                    step_breakdown["loopPenalty"] -= self.reward_config.loop_penalty

            slack = self._compute_slack()
            slack_delta = self.prev_slack - slack
            if self.reward_config.compact_weight != 0.0 and slack_delta != 0.0:
                compact_reward = slack_delta * self.reward_config.compact_weight
                reward += compact_reward
                step_breakdown["compactness"] += compact_reward
            self.last_slack_delta = slack_delta
            self.prev_slack = slack

            if len(self.snake) > self.max_length:
                gain = len(self.snake) - self.max_length
                growth_bonus = self.reward_config.growth_bonus * gain
                reward += growth_bonus
                step_breakdown["growthBonus"] += growth_bonus
                self.max_length = len(self.snake)

            if self.steps_since_fruit > self.total_cells * 2:
                reward -= self.reward_config.timeout_penalty
                step_breakdown["timeoutPenalty"] -= self.reward_config.timeout_penalty
                done = True
                cause = "timeout"

        step_breakdown["total"] = reward
        self.total_reward += reward
        self.steps_taken += 1

        for key, value in step_breakdown.items():
            if key == "total":
                continue
            self.episode_breakdown[key] += value
        self.episode_breakdown["total"] += reward
        self.reward_breakdown = {key: (self.episode_breakdown[key] if key != "total" else self.episode_breakdown["total"]) for key in self.episode_breakdown}

        if not done:
            next_state = build_state_vector(self.snake, self.fruit, self.direction_index, self.grid_size)
            self.state = next_state.copy()
        else:
            next_state = self.state.copy()

        info: Dict[str, object] = {
            "cause": cause,
            "breakdown": step_breakdown,
            "fruits": self.fruits_eaten,
            "steps": self.steps_taken,
        }
        return next_state, reward, done, info

    def _spawn_fruit(self) -> None:
        free_cells = [
            (x, y)
            for x in range(self.grid_size)
            for y in range(self.grid_size)
            if (x, y) not in self.snake_set
        ]
        self.fruit = self.random.choice(free_cells) if free_cells else self.snake[0]

    def snapshot(self) -> Dict[str, object]:
        """Return a shallow copy of the current game state for visualisering."""

        return {
            "snake": list(self.snake),
            "fruit": self.fruit,
            "direction_index": self.direction_index,
            "pending_growth": self.pending_growth,
        }


# ---------------------------------------------------------------------------
# Tkinter score tracking and rendering
# ---------------------------------------------------------------------------


@dataclass
class Score:
    reward_config: RewardConfig = field(default_factory=RewardConfig)
    fruits: int = 0
    steps: int = 0
    reward: float = 0.0

    def reset(self) -> None:
        self.fruits = 0
        self.steps = 0
        self.reward = 0.0

    def apply_step(self, outcome: str) -> None:
        self.steps += 1
        self.reward -= self.reward_config.step_penalty
        if outcome == "fruit":
            self.fruits += 1
            self.reward += self.reward_config.fruit_reward
        elif outcome == "wall":
            self.reward -= self.reward_config.wall_penalty
        elif outcome == "self":
            self.reward -= self.reward_config.self_penalty


class SnakeCanvas(tk.Canvas):
    """Canvas widget that hosts the actual snake game."""

    def __init__(self, master: tk.Misc, status: tk.Label, agent: Optional[DoubleDQNAgent] = None, autopilot: bool = False) -> None:
        pixel_size = GRID_SIZE * CELL_SIZE
        super().__init__(
            master,
            width=pixel_size,
            height=pixel_size,
            bg="#151515",
            highlightthickness=0,
        )
        self.pack()
        self.status_label = status
        self.score = Score()
        self._running = False
        self._after_id: Optional[str] = None
        self.agent = agent
        self.autopilot = autopilot and agent is not None
        self.reset()
        self.focus_set()
        self.bind("<KeyPress>", self.on_key_press)

    def reset(self) -> None:
        self.delete("all")
        start_x = GRID_SIZE // 2
        start_y = GRID_SIZE // 2
        self.snake = [(start_x - i, start_y) for i in range(START_LENGTH)]
        self.direction = (1, 0)
        self.score.reset()
        self._running = True
        self.spawn_fruit()
        self.draw_frame()
        self.update_status("start")

    def spawn_fruit(self) -> None:
        free_cells = [
            (x, y)
            for x in range(GRID_SIZE)
            for y in range(GRID_SIZE)
            if (x, y) not in self.snake
        ]
        self.fruit = random.choice(free_cells) if free_cells else self.snake[0]

    # Input handling -----------------------------------------------------
    def on_key_press(self, event: tk.Event[tk.Misc]) -> None:
        if event.keysym == "space":
            self.reset()
            return
        if event.keysym == "Escape":
            self.quit_game()
            return
        if event.keysym.lower() == "a":
            self.toggle_autopilot()
            return
        if event.keysym not in DIRECTIONS:
            return
        new_dir = DIRECTIONS[event.keysym]
        if OPPOSITE.get(self.direction) == new_dir:
            return
        self.direction = new_dir

    def toggle_autopilot(self) -> None:
        if self.agent is None:
            print("Ingen trÃ¤nad agent laddad. AnvÃ¤nd --load-model fÃ¶r att aktivera autopilot.")
            return
        self.autopilot = not self.autopilot
        mode = "pÃ¥" if self.autopilot else "av"
        print(f"Autopilot Ã¤r nu {mode}.")
        self.update_status("step")

    def quit_game(self) -> None:
        self._running = False
        if self._after_id is not None:
            self.after_cancel(self._after_id)
        self.master.destroy()

    # Rendering helpers --------------------------------------------------
    def draw_frame(self) -> None:
        self.delete("all")
        self.draw_grid()
        self.draw_snake()
        self.draw_fruit()

    def draw_grid(self) -> None:
        pixel_size = GRID_SIZE * CELL_SIZE
        for row in range(GRID_SIZE):
            color = "#1f1f1f" if row % 2 == 0 else "#232323"
            self.create_rectangle(
                0,
                row * CELL_SIZE,
                pixel_size,
                (row + 1) * CELL_SIZE,
                fill=color,
                outline=color,
            )

    def draw_snake(self) -> None:
        for index, (x, y) in enumerate(self.snake):
            fill = "#32c85c" if index == 0 else "#58e279"
            self._draw_cell(x, y, fill)

    def draw_fruit(self) -> None:
        fx, fy = self.fruit
        self._draw_cell(fx, fy, "#ff5c5c")

    def _draw_cell(self, x: int, y: int, color: str) -> None:
        self.create_rectangle(
            x * CELL_SIZE + 2,
            y * CELL_SIZE + 2,
            (x + 1) * CELL_SIZE - 2,
            (y + 1) * CELL_SIZE - 2,
            fill=color,
            outline="",
        )

    # Game loop ----------------------------------------------------------
    def start(self) -> None:
        self.after(300, self.tick)

    def tick(self) -> None:
        if not self._running:
            return
        if self.autopilot and self.agent is not None:
            direction_idx = DIRECTION_TO_INDEX[self.direction]
            state = build_state_vector(self.snake, self.fruit, direction_idx, GRID_SIZE)
            action = self.agent.select_action(state, greedy=True)
            chosen_direction = ACTION_VECTORS[action]
            if OPPOSITE[self.direction] != chosen_direction:
                self.direction = chosen_direction
        outcome = self.advance_snake()
        self.draw_frame()
        self.score.apply_step(outcome)
        self.update_status(outcome)
        if outcome in {"wall", "self"}:
            self._running = False
            self.update_status(outcome, finished=True)
            return
        self._after_id = self.after(STEP_DELAY, self.tick)

    def update_status(self, outcome: str, finished: bool = False) -> None:
        if outcome == "fruit":
            prefix = "ðŸŽ Frukt!"
        elif outcome == "wall":
            prefix = "ðŸ’¥ Krockade med vÃ¤ggen."
        elif outcome == "self":
            prefix = "ðŸ’¥ Ã…t sig sjÃ¤lv."
        elif outcome == "start":
            prefix = "ðŸ Nystart."
        else:
            prefix = "ðŸ"
        suffix = " Tryck Space fÃ¶r att bÃ¶rja om." if finished else ""
        autopilot_text = " ðŸ¤–" if self.autopilot and self.agent is not None else ""
        self.status_label.configure(
            text=(
                f"{prefix}{autopilot_text} PoÃ¤ng: {self.score.reward:.0f}  "
                f"Frukter: {self.score.fruits}  Steg: {self.score.steps}.{suffix}"
            )
        )

    def advance_snake(self) -> str:
        head_x, head_y = self.snake[0]
        dx, dy = self.direction
        new_head = (head_x + dx, head_y + dy)

        if not (0 <= new_head[0] < GRID_SIZE and 0 <= new_head[1] < GRID_SIZE):
            return "wall"
        if new_head in self.snake:
            return "self"

        self.snake.insert(0, new_head)
        if new_head == self.fruit:
            self.spawn_fruit()
            return "fruit"
        self.snake.pop()
        return "step"


class TrainingViewer:
    """Lightweight Tkinter-visualisering av trÃ¤ningsmiljÃ¶n."""

    def __init__(
        self,
        *,
        grid_size: int,
        total_episodes: int,
        steps_per_episode: int,
        cell_size: int = CELL_SIZE,
        delay_ms: int = STEP_DELAY,
    ) -> None:
        self.grid_size = grid_size
        self.total_episodes = total_episodes
        self.steps_per_episode = steps_per_episode
        self.cell_size = cell_size
        self.delay = max(delay_ms / 1000.0, 0.0)
        self._last_draw = 0.0
        self.closed = False

        self.root = tk.Tk()
        self.root.title("Snake-ML â€“ TrÃ¤ningsvisualisering")
        self.root.configure(bg="#101010")
        self.root.resizable(False, False)
        self.root.protocol("WM_DELETE_WINDOW", self.close)

        pixel_size = self.grid_size * self.cell_size
        margin = 16

        self.status = tk.Label(
            self.root,
            text="Initierar trÃ¤ningâ€¦",
            font=("Segoe UI", 12),
            anchor="w",
            padx=12,
            pady=8,
            width=80,
            bg="#101010",
            fg="#f5f5f5",
        )
        self.status.pack(fill="x")

        self.canvas = tk.Canvas(
            self.root,
            width=pixel_size,
            height=pixel_size,
            bg="#151515",
            highlightthickness=0,
        )
        self.canvas.pack(padx=margin, pady=margin)

        view_size = pixel_size + 2 * margin
        self.root.update_idletasks()
        status_height = self.status.winfo_height()
        total_height = view_size + status_height
        geometry = f"{view_size}x{total_height}"
        self.root.geometry(geometry)
        self.root.minsize(view_size, total_height)
        self.root.maxsize(view_size, total_height)

        self._draw_background()
        self.root.update_idletasks()
        self.root.update()

    def update(
        self,
        *,
        snapshot: Dict[str, object],
        episode: int,
        step: int,
        epsilon: float,
        total_reward: float,
        last_reward: float,
        cause: str,
        loss: float,
    ) -> None:
        if self.closed:
            return
        now = time.perf_counter()
        if now - self._last_draw < self.delay:
            self.root.update_idletasks()
            self.root.update()
            return
        self._last_draw = now

        snake = list(snapshot.get("snake", []))
        fruit = snapshot.get("fruit")

        self.canvas.delete("all")
        self._draw_background()
        self._draw_fruit(fruit)
        self._draw_snake(snake)

        status_text = (
            f"Episod {episode}/{self.total_episodes} | Steg {step} | "
            f"Reward {total_reward:6.1f} | Î” {last_reward:5.2f} | Îµ={epsilon:.3f} | "
            f"Senaste: {self._format_cause(cause)} | FÃ¶rlust {loss:7.4f}"
        )
        self.status.config(text=status_text)

        self.root.update_idletasks()
        self.root.update()

    def episode_done(self) -> None:
        if self.closed:
            return
        current = self.status.cget("text")
        if "| Slut" not in current:
            self.status.config(text=f"{current} | Slut pÃ¥ episod")
        self.root.update_idletasks()
        self.root.update()

    def close(self) -> None:
        if self.closed:
            return
        self.closed = True
        try:
            self.root.destroy()
        except tk.TclError:
            pass

    def _draw_background(self) -> None:
        pixel_size = self.grid_size * self.cell_size
        for row in range(self.grid_size):
            color = "#1f1f1f" if row % 2 == 0 else "#232323"
            self.canvas.create_rectangle(
                0,
                row * self.cell_size,
                pixel_size,
                (row + 1) * self.cell_size,
                fill=color,
                width=0,
            )

    def _draw_snake(self, snake: List[Point]) -> None:
        for index, (x, y) in enumerate(snake):
            color = "#8bc34a" if index == 0 else "#4caf50"
            self.canvas.create_rectangle(
                x * self.cell_size,
                y * self.cell_size,
                (x + 1) * self.cell_size,
                (y + 1) * self.cell_size,
                fill=color,
                outline="#1b5e20",
                width=1,
            )

    def _draw_fruit(self, fruit: Optional[Point]) -> None:
        if fruit is None:
            return
        x, y = fruit
        self.canvas.create_oval(
            x * self.cell_size + 4,
            y * self.cell_size + 4,
            (x + 1) * self.cell_size - 4,
            (y + 1) * self.cell_size - 4,
            fill="#ff9800",
            outline="#ef6c00",
            width=1,
        )

    def _format_cause(self, cause: str) -> str:
        translations = {
            "fruit": "frukt",
            "wall": "vÃ¤gg",
            "self": "kollision",
            "step": "steg",
        }
        return translations.get(cause, cause)


# ---------------------------------------------------------------------------
# Interaktiv trÃ¤ningskontroll fÃ¶r IDLE
# ---------------------------------------------------------------------------


@dataclass
class TrainingStats:
    episodes: int = 0
    total_reward: float = 0.0
    total_length: float = 0.0
    best_length: int = 0
    last_episode_reward: float = 0.0
    last_episode_length: float = 0.0

    def reset(self) -> None:
        self.episodes = 0
        self.total_reward = 0.0
        self.total_length = 0.0
        self.best_length = 0
        self.last_episode_reward = 0.0
        self.last_episode_length = 0.0

    def record(self, reward: float, length: float) -> None:
        self.episodes += 1
        self.total_reward += reward
        self.total_length += length
        self.best_length = max(self.best_length, int(length))
        self.last_episode_reward = reward
        self.last_episode_length = length

    @property
    def average_reward(self) -> float:
        return self.total_reward / self.episodes if self.episodes else 0.0

    @property
    def average_length(self) -> float:
        return self.total_length / self.episodes if self.episodes else 0.0


class IdleTrainerApp:
    """Tkinter-grÃ¤nssnitt fÃ¶r att styra trÃ¤ning och visa statistik live."""

    def __init__(self, *, load_path: Optional[Path | str] = None) -> None:
        self.env = IdleSnakeEnv()
        self.stats = TrainingStats()
        self.training_active = False
        self.last_reward: float = 0.0
        self.last_loss: Optional[float] = None
        self.current_state: Optional[np.ndarray] = None
        self.current_episode_reward: float = 0.0
        self.current_episode_steps: int = 0
        self._syncing_params = False
        self._pending_message: Optional[str] = None

        self.agent = self._load_initial_agent(load_path)

        self.root = tk.Tk()
        self.root.title("Snake-ML â€“ TrÃ¤ningskontroll")
        self.root.configure(bg="#101010")
        self.root.resizable(False, False)
        self.root.protocol("WM_DELETE_WINDOW", self._on_close)

        self._build_ui()
        self._reset_episode()
        self._sync_param_entries_from_agent()
        if self._pending_message:
            self._log_message(self._pending_message)
        else:
            self._log_message("Klar att starta trÃ¤ning.")
        self.root.after(50, self._training_loop)

    def _load_initial_agent(self, load_path: Optional[Path | str]) -> DoubleDQNAgent:
        if load_path is not None:
            try:
                agent = DoubleDQNAgent.load(load_path)
            except Exception as exc:  # pragma: no cover - anvÃ¤ndarfeedback
                print(f"Kunde inte ladda modellen '{load_path}': {exc}", file=sys.stderr)
            else:
                if agent.state_size != self.env.state_size:
                    print(
                        "Den laddade modellen matchar inte rutnÃ¤tets storlek och kan inte anvÃ¤ndas.",
                        file=sys.stderr,
                    )
                else:
                    name = Path(load_path).name if isinstance(load_path, (str, Path)) else "modell"
                    self._pending_message = f"Laddade modell frÃ¥n {name}."
                    return agent
            self._pending_message = "Misslyckades att ladda angiven modell. Startar med ny agent."
        return DoubleDQNAgent(state_size=self.env.state_size, action_size=len(ACTION_VECTORS))

    def _build_ui(self) -> None:
        main = tk.Frame(self.root, bg="#101010")
        main.pack(fill="both", expand=True, padx=16, pady=16)

        pixel_size = GRID_SIZE * CELL_SIZE
        self.canvas = tk.Canvas(
            main,
            width=pixel_size,
            height=pixel_size,
            bg="#151515",
            highlightthickness=0,
        )
        self.canvas.grid(row=0, column=0, rowspan=4, sticky="nsew")

        sidebar = tk.Frame(main, bg="#101010")
        sidebar.grid(row=0, column=1, sticky="nw", padx=(16, 0))

        self.stats_var = tk.StringVar()
        stats_label = tk.Label(
            sidebar,
            textvariable=self.stats_var,
            justify="left",
            wraplength=320,
            anchor="w",
            font=("Segoe UI", 11),
            bg="#101010",
            fg="#f5f5f5",
        )
        stats_label.pack(fill="x")

        self.detail_var = tk.StringVar()
        detail_label = tk.Label(
            sidebar,
            textvariable=self.detail_var,
            justify="left",
            wraplength=320,
            anchor="w",
            font=("Segoe UI", 10),
            bg="#101010",
            fg="#cddc39",
        )
        detail_label.pack(fill="x", pady=(4, 8))

        self._create_buttons(sidebar)

        params_container = tk.Frame(sidebar, bg="#101010")
        params_container.pack(fill="x", pady=(12, 8))

        header = tk.Label(
            params_container,
            text="Hyperparametrar",
            font=("Segoe UI", 11, "bold"),
            anchor="w",
            bg="#101010",
            fg="#f5f5f5",
        )
        header.pack(fill="x", pady=(0, 6))

        self.param_vars: Dict[str, tk.StringVar] = {}
        param_specs = [
            ("learning_rate", "LÃ¤rhastighet", float),
            ("gamma", "Gamma", float),
            ("epsilon", "Epsilon", float),
            ("epsilon_min", "Min epsilon", float),
            ("epsilon_decay", "Epsilon-fÃ¶rfall", float),
            ("batch_size", "Batch-storlek", int),
            ("target_sync_interval", "Synkintervall", int),
        ]
        for name, label, value_type in param_specs:
            row = tk.Frame(params_container, bg="#101010")
            row.pack(fill="x", pady=2)
            tk.Label(
                row,
                text=label,
                width=18,
                anchor="w",
                bg="#101010",
                fg="#d0d0d0",
                font=("Segoe UI", 10),
            ).pack(side="left")
            var = tk.StringVar()
            entry = tk.Entry(
                row,
                textvariable=var,
                width=10,
                justify="right",
                bg="#1c1c1c",
                fg="#f5f5f5",
                insertbackground="#f5f5f5",
                font=("Consolas", 10),
            )
            entry.pack(side="left", padx=(4, 0))
            var.trace_add("write", partial(self._on_param_change, name, var, value_type))
            self.param_vars[name] = var

        steps_frame = tk.Frame(params_container, bg="#101010")
        steps_frame.pack(fill="x", pady=(8, 0))
        tk.Label(
            steps_frame,
            text="Steg per uppdatering",
            anchor="w",
            bg="#101010",
            fg="#d0d0d0",
            font=("Segoe UI", 10),
        ).pack(side="left")
        self.steps_per_tick_var = tk.IntVar(value=4)
        steps_spin = tk.Spinbox(
            steps_frame,
            from_=1,
            to=256,
            textvariable=self.steps_per_tick_var,
            width=5,
            justify="right",
            bg="#1c1c1c",
            fg="#f5f5f5",
            insertbackground="#f5f5f5",
        )
        steps_spin.pack(side="left", padx=(8, 0))

        self.message_var = tk.StringVar()
        message_label = tk.Label(
            sidebar,
            textvariable=self.message_var,
            justify="left",
            wraplength=320,
            anchor="w",
            font=("Segoe UI", 10),
            bg="#101010",
            fg="#90caf9",
        )
        message_label.pack(fill="x", pady=(8, 0))

    def _create_buttons(self, parent: tk.Misc) -> None:
        button_frame = tk.Frame(parent, bg="#101010")
        button_frame.pack(fill="x", pady=(0, 8))

        btn_specs = [
            ("Starta trÃ¤ning", self.start_training),
            ("Pausa", self.pause_training),
            ("Spara modell", self.save_model),
            ("Ladda modell", self.load_model),
            ("Ã…terstÃ¤ll statistik", self.reset_stats),
        ]
        for text, command in btn_specs:
            button = tk.Button(
                button_frame,
                text=text,
                command=command,
                width=20,
                bg="#2e7d32" if "Starta" in text else "#455a64",
                fg="#f5f5f5",
                activebackground="#1b5e20",
                activeforeground="#f5f5f5",
                relief="flat",
                pady=6,
                font=("Segoe UI", 10, "bold" if "Starta" in text else "normal"),
            )
            button.pack(fill="x", pady=3)

    def _reset_episode(self) -> None:
        self.current_state = self.env.reset()
        self.current_episode_reward = 0.0
        self.current_episode_steps = 0
        self.last_reward = 0.0
        self._update_canvas(self.env.snapshot())
        self._update_stats_label()

    def _training_loop(self) -> None:
        if self.training_active and self.current_state is not None:
            try:
                steps = max(1, int(self.steps_per_tick_var.get()))
            except (TypeError, ValueError):
                steps = 1
            for _ in range(steps):
                if not self.training_active:
                    break
                self._run_training_step()
        self._update_canvas(self.env.snapshot())
        self._update_stats_label()
        self.root.after(50, self._training_loop)

    def _run_training_step(self) -> None:
        assert self.current_state is not None
        action = self.agent.select_action(self.current_state)
        next_state, reward, done, info = self.env.step(action)
        transition = Transition(
            state=self.current_state.copy(),
            action=action,
            reward=reward,
            next_state=next_state.copy(),
            done=done,
        )
        self.agent.push(transition)
        loss = self.agent.learn()
        if loss is not None:
            self.last_loss = loss
        self.agent.decay_epsilon()

        self.current_state = next_state
        self.current_episode_reward += reward
        self.current_episode_steps = int(info.get("steps", self.current_episode_steps + 1))
        self.last_reward = reward

        if done:
            self.stats.record(self.current_episode_reward, self.env.max_length)
            self._reset_episode()

    def _update_canvas(self, snapshot: Dict[str, object]) -> None:
        self.canvas.delete("all")
        self._draw_background()
        self._draw_fruit(snapshot.get("fruit"))
        snake = snapshot.get("snake") or []
        self._draw_snake(list(snake))

    def _draw_background(self) -> None:
        pixel_size = GRID_SIZE * CELL_SIZE
        for row in range(GRID_SIZE):
            color = "#1f1f1f" if row % 2 == 0 else "#232323"
            self.canvas.create_rectangle(
                0,
                row * CELL_SIZE,
                pixel_size,
                (row + 1) * CELL_SIZE,
                fill=color,
                width=0,
            )

    def _draw_snake(self, snake: List[Point]) -> None:
        for index, (x, y) in enumerate(snake):
            color = "#8bc34a" if index == 0 else "#4caf50"
            self.canvas.create_rectangle(
                x * CELL_SIZE,
                y * CELL_SIZE,
                (x + 1) * CELL_SIZE,
                (y + 1) * CELL_SIZE,
                fill=color,
                outline="#1b5e20",
                width=1,
            )

    def _draw_fruit(self, fruit: Optional[Point]) -> None:
        if fruit is None:
            return
        x, y = fruit
        self.canvas.create_oval(
            x * CELL_SIZE + 4,
            y * CELL_SIZE + 4,
            (x + 1) * CELL_SIZE - 4,
            (y + 1) * CELL_SIZE - 4,
            fill="#ff9800",
            outline="#ef6c00",
            width=1,
        )

    def _update_stats_label(self) -> None:
        avg_reward = self.stats.average_reward
        avg_length = self.stats.average_length
        best_length = self.stats.best_length
        current_length = len(self.env.snake)
        self.stats_var.set(
            (
                f"Episoder: {self.stats.episodes} | MedelpoÃ¤ng: {avg_reward:.2f} | "
                f"MedellÃ¤ngd: {avg_length:.2f} | LÃ¤ngsta lÃ¤ngd: {best_length} | "
                f"Nuvarande lÃ¤ngd: {current_length} | Senaste episodpoÃ¤ng: {self.stats.last_episode_reward:.2f} | "
                f"Senaste lÃ¤ngd: {self.stats.last_episode_length:.2f}"
            )
        )
        loss_text = f"{self.last_loss:.4f}" if self.last_loss is not None else "â€¦"
        self.detail_var.set(
            (
                f"Episod {self.stats.episodes + 1} | Steg {self.current_episode_steps} | "
                f"Nuvarande lÃ¤ngd: {len(self.env.snake)} | Max lÃ¤ngd: {self.env.max_length} | "
                f"Senaste reward: {self.last_reward:+.2f} | Episodreward: {self.current_episode_reward:.2f} | "
                f"FÃ¶rlust: {loss_text} | Îµ={self.agent.epsilon:.3f}"
            )
        )

    def _log_message(self, message: str) -> None:
        self.message_var.set(message)

    def start_training(self) -> None:
        if not self.training_active:
            self.training_active = True
            self._log_message("TrÃ¤ning pÃ¥gÃ¥râ€¦")

    def pause_training(self) -> None:
        if self.training_active:
            self.training_active = False
            self._log_message("TrÃ¤ningen pausades.")

    def reset_stats(self) -> None:
        self.stats.reset()
        self._log_message("Statistiken Ã¥terstÃ¤lldes.")
        self._update_stats_label()

    def load_model(self) -> None:
        path = filedialog.askopenfilename(
            title="VÃ¤lj modell att ladda",
            filetypes=[("Double DQN-modeller", "*.npz"), ("Alla filer", "*.*")],
        )
        if not path:
            return
        try:
            agent = DoubleDQNAgent.load(path)
        except Exception as exc:  # pragma: no cover - anvÃ¤ndarfeedback
            messagebox.showerror("Fel", f"Kunde inte ladda modellen:\n{exc}")
            return
        if agent.state_size != self.env.state_size:
            messagebox.showerror(
                "Fel",
                "Modellen anvÃ¤nder ett annat rutnÃ¤tsformat och kan inte laddas i denna miljÃ¶.",
            )
            return
        self.pause_training()
        self.agent = agent
        self._sync_param_entries_from_agent()
        self._reset_episode()
        name = Path(path).name
        self._log_message(f"Laddade modell frÃ¥n {name}.")

    def save_model(self) -> None:
        path = filedialog.asksaveasfilename(
            title="Spara Double DQN-modell",
            defaultextension=".npz",
            initialfile="idle_double_dqn.npz",
            filetypes=[("Double DQN-modeller", "*.npz"), ("Alla filer", "*.*")],
        )
        if not path:
            return
        try:
            saved_path = self.agent.save(path)
        except Exception as exc:  # pragma: no cover - anvÃ¤ndarfeedback
            messagebox.showerror("Fel", f"Kunde inte spara modellen:\n{exc}")
            return
        name = Path(saved_path).name
        self._log_message(f"Modellen sparades till {name}.")

    def _sync_param_entries_from_agent(self) -> None:
        self._syncing_params = True
        for name, var in self.param_vars.items():
            value = getattr(self.agent, name)
            var.set(self._format_param_value(value))
        self._syncing_params = False

    def _format_param_value(self, value: float | int) -> str:
        if isinstance(value, float):
            return f"{value:.6g}"
        return str(int(value))

    def _on_param_change(self, name: str, var: tk.StringVar, value_type: type, *_: object) -> None:
        if self._syncing_params:
            return
        text = var.get().strip()
        if not text:
            return
        try:
            value = value_type(text)
        except ValueError:
            return
        if isinstance(value, float) and not math.isfinite(value):
            return
        if value_type is int:
            value = max(1, int(value))
        setattr(self.agent, name, value)

    def _on_close(self) -> None:
        self.training_active = False
        try:
            self.root.destroy()
        except tk.TclError:
            pass

    def run(self) -> None:
        self.root.mainloop()


def start_idle_trainer_ui(load_path: Optional[Path | str] = None) -> None:
    """Starta det interaktiva Tk-grÃ¤nssnittet fÃ¶r trÃ¤ning."""

    app = IdleTrainerApp(load_path=load_path)
    app.run()


# ---------------------------------------------------------------------------
# Training / evaluation CLI
# ---------------------------------------------------------------------------


def train_double_dqn(
    episodes: int,
    steps_per_episode: int,
    *,
    seed: Optional[int] = None,
    load_path: Optional[Path | str] = None,
    save_path: Optional[Path | str] = None,
    visualize: bool = False,
) -> DoubleDQNAgent:
    env = IdleSnakeEnv(seed=seed)
    if load_path is not None:
        agent = DoubleDQNAgent.load(load_path)
        print(f"FortsÃ¤tter trÃ¤ning frÃ¥n {load_path}.")
    else:
        agent = DoubleDQNAgent(state_size=env.state_size, action_size=len(ACTION_VECTORS), seed=seed)
        print("Startar ny Double DQN-trÃ¤ning.")

    rewards: List[float] = []
    start_time = time.time()
    viewer: Optional[TrainingViewer] = None
    if visualize:
        try:
            viewer = TrainingViewer(
                grid_size=env.grid_size,
                total_episodes=episodes,
                steps_per_episode=steps_per_episode,
            )
        except tk.TclError as exc:
            print(
                "Det gick inte att starta trÃ¤ningsvisualiseringen (Tkinter). FortsÃ¤tter utan grafiskt lÃ¤ge.",
                file=sys.stderr,
            )
            viewer = None

    for episode in range(1, episodes + 1):
        state = env.reset()
        total_reward = 0.0
        losses: List[float] = []
        for step in range(steps_per_episode):
            action = agent.select_action(state)
            next_state, reward, done, info = env.step(action)
            agent.push(Transition(state, action, reward, next_state, done))
            loss = agent.learn()
            if loss is not None:
                losses.append(loss)
            agent.decay_epsilon()
            state = next_state
            total_reward += reward
            if viewer is not None:
                try:
                    viewer.update(
                        snapshot=env.snapshot(),
                        episode=episode,
                        step=step + 1,
                        epsilon=agent.epsilon,
                        total_reward=total_reward,
                        last_reward=reward,
                        cause=info.get("cause", "step"),
                        loss=losses[-1] if losses else 0.0,
                    )
                except tk.TclError:
                    print("TrÃ¤ningsfÃ¶nstret stÃ¤ngdes â€“ fortsÃ¤tter trÃ¤ningen utan visualisering.")
                    viewer = None
            if done:
                break
        rewards.append(total_reward)
        if episode % 10 == 0:
            mean_reward = sum(rewards[-10:]) / min(len(rewards), 10)
            mean_loss = sum(losses) / len(losses) if losses else 0.0
            duration = time.time() - start_time
            print(
                f"Episod {episode:4d} | SnittbelÃ¶ning (10): {mean_reward:6.2f} | "
                f"Senaste episod: {total_reward:6.2f} | FÃ¶rlust: {mean_loss:8.4f} | "
                f"Îµ={agent.epsilon:.3f} | Tid: {duration:5.1f}s"
            )
        if viewer is not None:
            try:
                viewer.episode_done()
            except tk.TclError:
                print("TrÃ¤ningsfÃ¶nstret stÃ¤ngdes â€“ fortsÃ¤tter trÃ¤ningen utan visualisering.")
                viewer = None

    if save_path is not None:
        path = agent.save(save_path)
        print(f"Sparade modellen till {path}.")
    if viewer is not None:
        viewer.close()
    return agent


def evaluate_agent(agent: DoubleDQNAgent, episodes: int, steps_per_episode: int, *, seed: Optional[int] = None) -> List[float]:
    env = IdleSnakeEnv(seed=seed)
    results: List[float] = []
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0.0
        for _ in range(steps_per_episode):
            action = agent.select_action(state, greedy=True)
            state, reward, done, _ = env.step(action)
            total_reward += reward
            if done:
                break
        results.append(total_reward)
    return results


def start_game(agent: Optional[DoubleDQNAgent] = None, autopilot: bool = False) -> None:
    root = tk.Tk()
    root.title("Snake-ML â€“ IDLE Edition")
    root.resizable(False, False)
    root.configure(bg="#101010")

    status = tk.Label(
        root,
        text="Tryck pÃ¥ piltangenterna fÃ¶r att styra. Space startar om, Escape avslutar.",
        font=("Segoe UI", 12),
        bg="#101010",
        fg="#f5f5f5",
        anchor="w",
        padx=12,
        pady=8,
    )
    status.pack(fill="x")

    canvas = SnakeCanvas(root, status, agent=agent, autopilot=autopilot)
    canvas.pack()
    canvas.start()

    root.mainloop()


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Snake-ML IDLE Double DQN toolkit")
    parser.add_argument("--train", type=int, metavar="EP", help="Antal trÃ¤ningsepisoder att kÃ¶ra", default=0)
    parser.add_argument("--steps", type=int, metavar="N", help="Maxsteg per episod under trÃ¤ning", default=500)
    parser.add_argument("--seed", type=int, help="SlumpfrÃ¶", default=None)
    parser.add_argument("--load-model", type=str, help="Ladda en sparad agent fÃ¶r trÃ¤ning eller spel", default=None)
    parser.add_argument("--save-model", type=str, help="Var ska modellen sparas efter trÃ¤ning", default="idle_double_dqn.npz")
    parser.add_argument("--evaluate", type=int, metavar="EP", help="KÃ¶r utvÃ¤rdering med angivet antal episoder", default=0)
    parser.add_argument("--play", action="store_true", help="Starta Tkinter-spelet")
    parser.add_argument("--autopilot", action="store_true", help="Aktivera autopilot nÃ¤r spelet startas")
    parser.add_argument(
        "--visualize-training",
        action="store_true",
        help="Visa trÃ¤ningsmiljÃ¶n live i ett Tkinter-fÃ¶nster",
    )
    return parser.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> None:
    args = parse_args(argv)

    agent: Optional[DoubleDQNAgent] = None
    if args.train > 0:
        agent = train_double_dqn(
            args.train,
            args.steps,
            seed=args.seed,
            load_path=args.load_model,
            save_path=args.save_model,
            visualize=args.visualize_training,
        )
    if args.evaluate or args.autopilot:
        if agent is None:
            if args.load_model:
                agent = DoubleDQNAgent.load(args.load_model)
                print(f"Laddade agent frÃ¥n {args.load_model}.")
            else:
                raise SystemExit("Ingen agent att anvÃ¤nda. TrÃ¤na fÃ¶rst eller ladda en modell.")

    if args.evaluate:
        scores = evaluate_agent(agent, args.evaluate, args.steps, seed=args.seed)
        avg = sum(scores) / len(scores)
        best = max(scores)
        worst = min(scores)
        print(
            f"UtvÃ¤rdering Ã¶ver {args.evaluate} episoder | Snitt: {avg:.2f} | "
            f"BÃ¤st: {best:.2f} | SÃ¤mst: {worst:.2f}"
        )

    if args.play or (args.autopilot and agent is not None):
        try:
            start_game(agent=agent, autopilot=args.autopilot)
        except tk.TclError as exc:  # pragma: no cover - headless safeguard
            print(
                "Det gick inte att starta Tkinter. Om du kÃ¶r skriptet pÃ¥ en server utan skÃ¤rm",
                "behÃ¶ver du kÃ¶ra det lokalt i stÃ¤llet.",
                file=sys.stderr,
            )
            raise SystemExit(1) from exc
    elif args.train == 0 and args.evaluate == 0:
        # Default behaviour when running without flaggar: visa den nya trÃ¤ningskontrollen.
        try:
            start_idle_trainer_ui(load_path=args.load_model if agent is None else None)
        except tk.TclError as exc:  # pragma: no cover - headless safeguard
            print(
                "Det gick inte att starta Tkinter. Om du kÃ¶r skriptet pÃ¥ en server utan skÃ¤rm",
                "behÃ¶ver du kÃ¶ra det lokalt i stÃ¤llet.",
                file=sys.stderr,
            )
            raise SystemExit(1) from exc


if __name__ == "__main__":
    main()
